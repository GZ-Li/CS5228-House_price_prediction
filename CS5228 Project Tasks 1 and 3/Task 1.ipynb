{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4173499",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9c6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrt_stations = pd.read_csv(\"auxiliary-data/sg-mrt-stations.csv\")\n",
    "primary_schools = pd.read_csv(\"auxiliary-data/sg-primary-schools.csv\")\n",
    "shopping_malls = pd.read_csv(\"auxiliary-data/sg-shopping-malls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b43f9",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the data with 0 price\n",
    "df_train = df_train[df_train[\"price\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b80523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na with the nearest data or data from the same subzone\n",
    "def min_dist_feature_in_same_subzone (fill_in_feature, subzone, lat, lng, df):\n",
    "    df_subset = df[df['subzone'] == subzone]\n",
    "    df_subset = df_subset.reset_index(drop=True)\n",
    "    return df.iloc[np.argmin(np.sqrt((df[\"lat\"]-lat)**2+(df[\"lng\"]-lng)**2))][fill_in_feature]\n",
    "\n",
    "def fill_NA_with_nearest_record(df, empty_feature):\n",
    "    df_empty = df[(df[empty_feature].isna())]\n",
    "    df_empty[empty_feature] = df_empty.apply(lambda row: min_dist_feature_in_same_subzone(empty_feature, row['subzone'], row['lat'], row['lng'], df[(df[empty_feature].notna())]), axis=1)\n",
    "    return df_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_type: First character uppercase -> lowercase\n",
    "df_test['property_type'] = df_test['property_type'].str.lower()\n",
    "# df_test.head()\n",
    "\n",
    "# property_type: (hdb 2 rooms, hdb 3 rooms, hdb 4 rooms, hdb 5 rooms) -> hdb\n",
    "temp = df_test['property_type'].str.startswith(('hdb 2', 'hdb 3', 'hdb 4', 'hdb 5'))\n",
    "df_test['property_type'] = np.where((temp == True), 'hdb', df_test['property_type'])\n",
    "\n",
    "# tenure: fill all hdb property_type with hdb defult tenure value - '99-year-leasehold'\n",
    "hdb_tenure = df_test[(df_test['property_type'].str.startswith('hdb')) & df_test['tenure'].notna() ]['tenure'].unique()[0]\n",
    "df_test['tenure'] = np.where((df_test['property_type'].str.startswith('hdb')) & (df_test['tenure'].isna()), hdb_tenure, df_test['tenure'])\n",
    "\n",
    "# tenure: fill in NaN tenure with value from same address or property_name, otherwise fill in value from the nearest property in the same subzone\n",
    "df_test.tenure = df_test.groupby('property_name').tenure.transform('first')\n",
    "df_test.tenure = df_test.groupby('address').tenure.transform('first')\n",
    "\n",
    "df_train_empty_tenure_filled = fill_NA_with_nearest_record(df_test, 'tenure')\n",
    "df_test.loc[df_test.listing_id.isin(df_train_empty_tenure_filled.listing_id), ['tenure']] = df_train_empty_tenure_filled[['tenure']]\n",
    "\n",
    "\n",
    "# built_year: fill in NaN built_year with value from same property_name or address, otherwise fill in with the nearest location record within the same subzone\n",
    "df_test.built_year = df_test.groupby('property_name').built_year.transform('first')\n",
    "df_test.built_year = df_test.groupby('address').built_year.transform('first')\n",
    "\n",
    "df_train_empty_built_year_filled = fill_NA_with_nearest_record(df_test, 'built_year')\n",
    "df_test.loc[df_test.listing_id.isin(df_train_empty_built_year_filled.listing_id), ['built_year']] = df_train_empty_built_year_filled[['built_year']]\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7fb636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_type: First character uppercase -> lowercase\n",
    "df_train['property_type'] = df_train['property_type'].str.lower()\n",
    "\n",
    "# property_type: (hdb 2 rooms, hdb 3 rooms, hdb 4 rooms, hdb 5 rooms) -> hdb\n",
    "temp = df_train['property_type'].str.startswith(('hdb 2', 'hdb 3', 'hdb 4', 'hdb 5'))\n",
    "df_train['property_type'] = np.where((temp == True), 'hdb', df_train['property_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_beds: fill 1 with studio\n",
    "df_train['num_beds'] = np.where((df_train['num_beds'].isna() & df_train['title'].str.startswith('studio ')), \n",
    "                                1, df_train['num_beds'])\n",
    "df_test['num_beds'] = np.where((df_test['num_beds'].isna() & df_test['title'].str.startswith('studio ')), \n",
    "                                1, df_test['num_beds'])\n",
    "df_test[df_test['num_beds'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef4495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price: delete rows with price value 0\n",
    "df_train = df_train[df_train['price'] != 0]\n",
    "\n",
    "# subzone & planning_area: delete rows with subzone and planning_area values NaN \n",
    "df_train = df_train[(df_train['subzone'].notna() & df_train['planning_area'].notna())]\n",
    "\n",
    "# tenure: fill all hdb property_type with hdb defult tenure value - '99-year-leasehold'\n",
    "hdb_tenure = df_train[(df_train['property_type'].str.startswith('hdb')) & df_train['tenure'].notna() ]['tenure'].unique()[0]\n",
    "df_train['tenure'] = np.where((df_train['property_type'].str.startswith('hdb')) & (df_train['tenure'].isna()), hdb_tenure, df_train['tenure'])\n",
    "\n",
    "# tenure: fill in NaN tenure with value from same property_name, otherwise drop\n",
    "df_train.tenure = df_train.groupby('property_name').tenure.transform('first')\n",
    "\n",
    "# tenure: delete rows with tenure values NaN as no useful records can be used to fill in NaN values\n",
    "df_train = df_train[df_train['tenure'].notna()]\n",
    "\n",
    "# built_year: fill in NaN built_year with value from same property_name, otherwise drop\n",
    "df_train.built_year = df_train.groupby('property_name').built_year.transform('first')\n",
    "\n",
    "df_train = df_train[df_train['built_year'].notna()]\n",
    "\n",
    "df_train['lease_end_year'] = df_train.tenure.str.extract('(\\d+)')\n",
    "df_train['lease_end_year'] = np.where((df_train['tenure'] == 'freehold'), 9999, df_train['lease_end_year'])\n",
    "df_train['lease_end_year'] = np.where((df_train['tenure'] == 'freehold'), 9999, df_train['lease_end_year'].astype(int) + df_train.built_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a64ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process the subzones in the test set that haven't appeared in the train set.\n",
    "for i in range(df_test.shape[0]):\n",
    "    if (df_test[\"subzone\"].isnull()[i] == True) or (df_test[\"subzone\"][i] not in list(df_train[\"subzone\"])):\n",
    "        temp = df_test.loc[i]\n",
    "        temp_lat = temp[\"lat\"]\n",
    "        temp_lng = temp[\"lng\"]\n",
    "        index = np.argmin(np.sqrt((df_train[\"lat\"]-temp_lat)**2+(df_train[\"lng\"]-temp_lng)**2))\n",
    "        df_test[\"subzone\"][i] = df_train.loc[index][\"subzone\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbd4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the data with unreasonable size\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Reset index\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "# Remove NaN in num_beds and num_baths\n",
    "df_train = df_train.dropna(subset=['num_beds'])\n",
    "df_train = df_train.dropna(subset=['num_baths'])\n",
    "\n",
    "# DBSCAN using beds to baths ratio\n",
    "df_train['beds_to_baths'] = df_train['num_beds'] / df_train['num_baths']\n",
    "sk_clustering_iris = DBSCAN(eps=0.5, min_samples=5).fit(df_train[['beds_to_baths']])\n",
    "sk_noise_iris = np.argwhere(sk_clustering_iris.labels_ < 0).squeeze()\n",
    "sk_noise_iris.sort()\n",
    "\n",
    "for i in sk_noise_iris:\n",
    "    df_train = df_train.drop(i)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "# DBSCAN using baths to beds ratio\n",
    "df_train['baths_to_beds'] = df_train['num_baths'] / df_train['num_beds']\n",
    "sk_clustering_iris = DBSCAN(eps=0.5, min_samples=5).fit(df_train[['baths_to_beds']])\n",
    "sk_noise_iris = np.argwhere(sk_clustering_iris.labels_ < 0).squeeze()\n",
    "sk_noise_iris.sort()\n",
    "\n",
    "for i in sk_noise_iris:\n",
    "    df_train = df_train.drop(i)\n",
    "    df_train = df_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the data with unreasonable size\n",
    "# DBSCAN using size to rooms ratio\n",
    "df_train['sqft_to_rooms'] = df_train['size_sqft'] / (df_train['num_beds'] + df_train['num_baths'])\n",
    "sk_clustering_iris = DBSCAN(eps=50, min_samples=5).fit(df_train[['sqft_to_rooms']])\n",
    "sk_noise_iris = np.argwhere(sk_clustering_iris.labels_ < 0).squeeze()\n",
    "sk_noise_iris.sort()\n",
    "\n",
    "for i in sk_noise_iris:\n",
    "    df_train = df_train.drop(i)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# DBSCAN using rooms to size ratio\n",
    "df_train['rooms_to_sqft'] = (df_train['num_beds'] + df_train['num_baths']) / df_train['size_sqft']\n",
    "sk_clustering_iris = DBSCAN(eps=0.0005, min_samples=5).fit(df_train[['rooms_to_sqft']])\n",
    "sk_noise_iris = np.argwhere(sk_clustering_iris.labels_ < 0).squeeze()\n",
    "sk_noise_iris.sort()\n",
    "\n",
    "for i in sk_noise_iris:\n",
    "    df_train = df_train.drop(i)\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "\n",
    "df_train.drop(['rooms_to_sqft', 'sqft_to_rooms', 'baths_to_beds', 'beds_to_baths'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use new feature price_per_sqft to detect unreasonable price\n",
    "df_train[\"price_per_sqft\"] = df_train[\"price\"]/df_train[\"size_sqft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813fb19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "print(df_train[\"price_per_sqft\"].describe())\n",
    "sns.boxplot(y = df_train[\"price_per_sqft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unreasonable data based on 3-sigma rules\n",
    "while True:\n",
    "    mean = np.mean(df_train[\"price_per_sqft\"])\n",
    "    std = np.std(df_train[\"price_per_sqft\"])\n",
    "    high = mean + 3*std\n",
    "    low = mean - 3*std\n",
    "    if ((df_train[\"price_per_sqft\"]>low).all() and (df_train[\"price_per_sqft\"]<high).all()) == True:\n",
    "        break\n",
    "    else:\n",
    "        df_train = df_train[df_train[\"price_per_sqft\"] > low]\n",
    "        df_train = df_train[df_train[\"price_per_sqft\"] < high]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train[\"price_per_sqft\"].describe())\n",
    "sns.boxplot(y = df_train[\"price_per_sqft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a8684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a boundary to remove the unreasonably small data\n",
    "fig, ax =plt.subplots(1,3,constrained_layout=True, figsize=(12, 3))\n",
    "s1=sns.distplot(df_train[\"price_per_sqft\"], ax=ax[0])\n",
    "s1.set_title(\"all data\")\n",
    "s2=sns.distplot(df_train[df_train[\"price_per_sqft\"]<1000][\"price_per_sqft\"], ax=ax[1])\n",
    "s2.set_title(\"<1000\")\n",
    "s3=sns.distplot(df_train[df_train[\"price_per_sqft\"]<400][\"price_per_sqft\"], ax=ax[2])\n",
    "s3.set_title(\"<400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unreasonable data still exists, and drop them.\n",
    "df_train = df_train[df_train[\"price_per_sqft\"] > 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature num_rooms\n",
    "df_train[\"num_rooms\"] = df_train[\"num_beds\"] + df_train[\"num_baths\"]\n",
    "df_test[\"num_rooms\"] = df_test[\"num_beds\"] + df_test[\"num_baths\"]\n",
    "\n",
    "#Fill num_rooms NA value\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import floor\n",
    "regressor0 = LinearRegression()\n",
    "regressor0 = regressor0.fit(np.array(df_train[df_train[\"num_rooms\"].notnull()][\"size_sqft\"]).reshape(-1, 1), np.array(df_train[df_train[\"num_rooms\"].notnull()][\"num_rooms\"]).reshape(-1, 1))\n",
    "for i in range(df_test.shape[0]):\n",
    "    if df_test[\"num_rooms\"].isnull()[i] == True:\n",
    "        df_test.iloc[i, -1] =floor(regressor0.predict(np.array(df_test[\"size_sqft\"][i]).reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720dc001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop = True)\n",
    "df_test = df_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628bccef",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a3c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "train = copy.deepcopy(df_train)\n",
    "test = copy.deepcopy(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ca77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "temp = train.groupby(\"property_type\").mean()\n",
    "temp = temp.sort_values(by=\"price\",ascending=True)\n",
    "ax_order = temp.index.tolist()\n",
    "fig, ax =plt.subplots(1,2,constrained_layout=True, figsize=(12, 5))\n",
    "s1 = sns.barplot(y=\"price\", x=\"property_type\", data=train, ax=ax[0])\n",
    "s1.set_xticklabels(s1.get_xticklabels(),rotation = 80)\n",
    "s2 = sns.barplot(y=\"price\", x=\"property_type\", data=train, order=ax_order, ax=ax[1])\n",
    "s2.set_xticklabels(s2.get_xticklabels(),rotation = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding property_type based on the visualization above\n",
    "train = train.replace([\"hdb\", \"hdb executive\", \"walk-up\", \"executive condo\", \"shophouse\"],[0,0,0,0,0])\n",
    "train = train.replace([\"condo\", \"apartment\", \"landed\", \"terraced house\", \"cluster house\"],[1,1,1,1,1])\n",
    "train = train.replace([\"townhouse\", \"corner terrace\", \"good class bungalow\", \"semi-detached house\"],[2, 2, 2, 2])\n",
    "train = train.replace([\"bungalow\"], [3])\n",
    "\n",
    "test = test.replace([\"hdb\", \"hdb executive\", \"walk-up\", \"executive condo\", \"shophouse\"],[0,0,0,0,0])\n",
    "test = test.replace([\"condo\", \"apartment\", \"landed\", \"terraced house\", \"cluster house\"],[1,1,1,1,1])\n",
    "test = test.replace([\"townhouse\", \"corner terrace\", \"good class bungalow\", \"semi-detached house\"],[2, 2, 2, 2])\n",
    "test = test.replace([\"bungalow\", \"conservation house\"], [3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the tenure\n",
    "train = train.replace([\"99-year leasehold\", \"110-year leasehold\", \"103-year leasehold\", \"102-year leasehold\", \"100-year leasehold\"],[0,0,0,0,0])\n",
    "train = train.replace([\"999-year leasehold\", \"946-year leasehold\", \"956-year leasehold\", \"929-year leasehold\", \"947-year leasehold\"],[1,1,1,1,1])\n",
    "train = train.replace([\"freehold\"],[2])\n",
    "test = test.replace([\"99-year leasehold\", \"110-year leasehold\", \"103-year leasehold\", \"102-year leasehold\", \"100-year leasehold\"],[0,0,0,0,0])\n",
    "test = test.replace([\"999-year leasehold\", \"946-year leasehold\", \"956-year leasehold\", \"929-year leasehold\", \"947-year leasehold\"],[1,1,1,1,1])\n",
    "test = test.replace([\"freehold\"],[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c64508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the built_year\n",
    "#discrete\n",
    "#0-1963-1970\n",
    "#1-1971-1990\n",
    "#2-1991-2005\n",
    "#3-2006-2020\n",
    "#4-2021-2028\n",
    "for i in range(train.shape[0]):\n",
    "    if train.iloc[i,6]<=1970:\n",
    "        train.iloc[i,6]=0\n",
    "    if train.iloc[i,6]<=1990 and train.iloc[i,6]>=1971:\n",
    "        train.iloc[i,6]=1\n",
    "    if train.iloc[i,6]>=1991 and train.iloc[i,6]<=2005:\n",
    "        train.iloc[i,6]=2\n",
    "    if train.iloc[i,6]>=2006 and train.iloc[i,6]<=2020:\n",
    "        train.iloc[i,6]=3\n",
    "    if train.iloc[i,6]>=2021:\n",
    "        train.iloc[i,6]=4\n",
    "for i in range(df_test.shape[0]):\n",
    "    if df_test.iloc[i,6]<=1970:\n",
    "        df_test.iloc[i,6]=0\n",
    "    if df_test.iloc[i,6]<=1990 and df_test.iloc[i,6]>=1971:\n",
    "        df_test.iloc[i,6]=1\n",
    "    if df_test.iloc[i,6]>=1991 and df_test.iloc[i,6]<=2005:\n",
    "        df_test.iloc[i,6]=2\n",
    "    if df_test.iloc[i,6]>=2006 and df_test.iloc[i,6]<=2020:\n",
    "        df_test.iloc[i,6]=3\n",
    "    if df_test.iloc[i,6]>=2021:\n",
    "        df_test.iloc[i,6]=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836178ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to find the mininum distance, to deal with the auxilary data.\n",
    "def min_dist(lat, lng, df):\n",
    "    return min(np.sqrt((df[\"lat\"]-lat)**2+(df[\"lng\"]-lng)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3014d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the distance of nearest mrt station, primary school and shopping mall.\n",
    "min_dist_mrt = []\n",
    "min_dist_pri = []\n",
    "min_dist_mall = []\n",
    "for i in range(train.shape[0]):\n",
    "    min_dist_mrt.append(min_dist(train[\"lat\"][i],train[\"lng\"][i],mrt_stations))\n",
    "    min_dist_pri.append(min_dist(train[\"lat\"][i],train[\"lng\"][i],primary_schools))\n",
    "    min_dist_mall.append(min_dist(train[\"lat\"][i],train[\"lng\"][i],shopping_malls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9f38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "train[\"dist_mrt\"] = (np.array(min_dist_mrt)-min(min_dist_mrt))/(max(min_dist_mrt)-min(min_dist_mrt))\n",
    "train[\"dist_pri\"] = (np.array(min_dist_pri)-min(min_dist_pri))/(max(min_dist_pri)-min(min_dist_pri))\n",
    "train[\"dist_mall\"] = (np.array(min_dist_mall)-min(min_dist_mall))/(max(min_dist_mall)-min(min_dist_mall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a1a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the distance of nearest mrt station, primary school and shopping mall.\n",
    "min_dist_mrt = []\n",
    "min_dist_pri = []\n",
    "min_dist_mall = []\n",
    "for i in range(test.shape[0]):\n",
    "    min_dist_mrt.append(min_dist(test[\"lat\"][i],test[\"lng\"][i],mrt_stations))\n",
    "    min_dist_pri.append(min_dist(test[\"lat\"][i],test[\"lng\"][i],primary_schools))\n",
    "    min_dist_mall.append(min_dist(test[\"lat\"][i],test[\"lng\"][i],shopping_malls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbbd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "test[\"dist_mrt\"] = (np.array(min_dist_mrt)-min(min_dist_mrt))/(max(min_dist_mrt)-min(min_dist_mrt))\n",
    "test[\"dist_pri\"] = (np.array(min_dist_pri)-min(min_dist_pri))/(max(min_dist_pri)-min(min_dist_pri))\n",
    "test[\"dist_mall\"] = (np.array(min_dist_mall)-min(min_dist_mall))/(max(min_dist_mall)-min(min_dist_mall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the useless features\n",
    "train = train.drop(columns = [\"listing_id\", \"title\", \"address\", \"property_name\", \"num_beds\", \"num_baths\", \"floor_level\", \"available_unit_types\", \"total_num_units\", \"property_details_url\", \"elevation\", \"planning_area\", \"lease_end_year\", \"price_per_sqft\", \"furnishing\"])\n",
    "test = test.drop(columns = [\"listing_id\", \"title\", \"address\", \"property_name\", \"num_beds\", \"num_baths\", \"floor_level\", \"furnishing\", \"available_unit_types\", \"total_num_units\", \"property_details_url\", \"elevation\", \"planning_area\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf864f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train.groupby(\"subzone\").mean()\n",
    "temp = temp.sort_values(by=\"price\",ascending=True)\n",
    "ax_order = temp.index.tolist()\n",
    "fig, ax =plt.subplots(2, 1,constrained_layout=True, figsize=(60, 40))\n",
    "s1 = sns.barplot(y=\"price\", x=\"subzone\", data=train, ax=ax[0])\n",
    "s1.set_xticklabels(s1.get_xticklabels(),rotation = 80)\n",
    "s2 = sns.barplot(y=\"price\", x=\"subzone\", data=train, order=ax_order, ax=ax[1])\n",
    "s2.set_xticklabels(s2.get_xticklabels(),rotation = 80)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.xlabel(\"Subzone\", fontsize = 30)\n",
    "plt.ylabel(\"Average Price\", fontsize = 30)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.xlabel(\"Subzone\", fontsize = 30)\n",
    "plt.ylabel(\"Average Price\", fontsize = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f15574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the average price of all the houses in a subzone to encode the feature subzone\n",
    "for i in list(set(train[\"subzone\"])):\n",
    "    temp = train[train[\"subzone\"] == i]\n",
    "    train = train.replace(i, np.mean(temp[\"price\"]))\n",
    "    test = test.replace(i, np.mean(temp[\"price\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d0c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c86e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the input data and labels\n",
    "X_train = train.drop(columns = [\"price\"])\n",
    "y_train = train[\"price\"]\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303fa461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "X_train_N = copy.deepcopy(X_train)\n",
    "X_test_N = copy.deepcopy(X_test)\n",
    "for i in list(X_train_N.columns):\n",
    "    X_train_N[i] = (X_train[i]-min(X_train[i]))/(max(X_train[i]) - min(X_train[i]))\n",
    "for i in list(X_test_N.columns):\n",
    "    X_test_N[i] = (X_test[i]-min(X_test[i]))/(max(X_test[i]) - min(X_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(X_train).to_csv(\"X_train.csv\")\n",
    "#pd.DataFrame(X_test).to_csv(\"X_test.csv\")\n",
    "#pd.DataFrame(y_train).to_csv(\"y_train.csv\")\n",
    "#pd.DataFrame(X_train_N).to_csv(\"X_train_N.csv\")\n",
    "#pd.DataFrame(X_test_N).to_csv(\"X_test_N.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9afea",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d27d06",
   "metadata": {},
   "source": [
    "#### Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372fee3",
   "metadata": {},
   "source": [
    "##### Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d428d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()\n",
    "\n",
    "start = time.time()\n",
    "score = cross_val_score(regressor, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"The score for Linear Regression: {}\".format(-np.mean(score)*10))\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)/10))\n",
    "\n",
    "regressor = regressor.fit(X_train, y_train)\n",
    "y_predict = regressor.predict(X_test)\n",
    "pd.DataFrame(y_predict).to_csv(\"predictions/LinearRegression_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5014f6",
   "metadata": {},
   "source": [
    "##### Lasso Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66481c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'alpha': np.linspace(5, 600, 50)}]\n",
    "\n",
    "regressor = Lasso()\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf49ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best model is: {} \\n\".format(grid_search.best_estimator_))\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7340bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Lasso(alpha=442.1428571428571)\n",
    "\n",
    "start = time.time()\n",
    "score = cross_val_score(regressor, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"The score for Lasso Regression: {}\".format(-np.mean(score)))\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)/10))\n",
    "\n",
    "regressor = regressor.fit(X_train, y_train)\n",
    "y_predict = regressor.predict(X_test)\n",
    "pd.DataFrame(y_predict).to_csv(\"predictions/Lasso_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cccc87",
   "metadata": {},
   "source": [
    "##### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc5bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'alpha': np.linspace(0.0000000001, 10, 50)}]\n",
    "\n",
    "regressor = Ridge()\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6b5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best model is: {} \\n\".format(grid_search.best_estimator_))\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Ridge(alpha=1.8367346939591838) \n",
    "\n",
    "start = time.time()\n",
    "score = cross_val_score(regressor, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"The score for Ridge Regression: {}\".format(-np.mean(score)))\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)/10))\n",
    "\n",
    "regressor = regressor.fit(X_train, y_train)\n",
    "y_predict = regressor.predict(X_test)\n",
    "pd.DataFrame(y_predict).to_csv(\"predictions/Ridge_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a40af",
   "metadata": {},
   "source": [
    "#### Tree Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f30f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a54ef",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f98138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search(Change the param_grid to tune the parameters)\n",
    "\n",
    "#param_grid = [{'n_estimators': [50, 100, 150, 200, 250, 300], 'max_depth': [50, 100, 150, 200, 250, 300], 'min_samples_split': [2, 4, 6, 8, 10]}]\n",
    "#param_grid = [{'n_estimators': [70, 80, 90, 100, 110, 120, 130], 'max_depth': [30, 40, 50, 60, 70, 80], 'min_samples_split': [2]}]\n",
    "param_grid = [{'n_estimators': [10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250], 'max_depth': [230], 'min_samples_split': [2]}]\n",
    "\n",
    "#param_grid = [{'n_estimators': [100], 'max_depth': [10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250], 'min_samples_split': [2]}]\n",
    "\n",
    "\n",
    "regressor = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "print(\"The best model is: {} \\n\".format(grid_search.best_estimator_))\n",
    "\n",
    "RF_score = []\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))\n",
    "    RF_score.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ef8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-fold cross validation\n",
    "regressor = RandomForestRegressor(max_depth = 230, n_estimators = 100)\n",
    "\n",
    "start = time.time()\n",
    "score = cross_val_score(regressor, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"The score for RandomForest: {}\".format(-np.mean(score)))\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)/10))\n",
    "\n",
    "regressor = regressor.fit(X_train, y_train)\n",
    "y_predict = regressor.predict(X_test)\n",
    "pd.DataFrame(y_predict).to_csv(\"predictions/RandomForest_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aee85d",
   "metadata": {},
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268dff6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch(Change the param_grid to tune the parameters)\n",
    "\n",
    "#param_grid = [{'max_depth':[10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250], 'min_samples_split': [2, 4, 6, 8, 10]}]\n",
    "param_grid = [{'max_depth': [10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250], 'min_samples_split': [6]}]\n",
    "\n",
    "regressor = DecisionTreeRegressor(splitter = \"best\")\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f74fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "print(\"The best model is: {} \\n\".format(grid_search.best_estimator_))\n",
    "\n",
    "DT_score = []\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))\n",
    "    DT_score.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deca3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-fold cross validation\n",
    "regressor = DecisionTreeRegressor(max_depth = 110, min_samples_split = 6)\n",
    "\n",
    "start = time.time()\n",
    "score = cross_val_score(regressor, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"The score for RandomForest: {}\".format(-np.mean(score)))\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)/10))\n",
    "\n",
    "regressor = regressor.fit(X_train, y_train)\n",
    "y_predict = regressor.predict(X_test)\n",
    "pd.DataFrame(y_predict).to_csv(\"predictions/DecistionTree_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be73f42",
   "metadata": {},
   "source": [
    "##### Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282555d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearch(Change the param_grid to tune the parameters)\n",
    "\n",
    "#param_grid = [{'leaning_rate':[0.1, 0.01, 0.001, 0.0001], 'max_depth': [50, 100, 150, 200, 250, 300], \"n_estimators\": [50, 100, 150, 200, 250, 300]}]\n",
    "param_grid = [{'learning_rate': [0.1], 'max_depth': [10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250]}]\n",
    "\n",
    "regressor = GradientBoostingRegressor(n_estimators = 100)\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae16c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "print(\"The best model is: {} \\n\".format(grid_search.best_estimator_))\n",
    "\n",
    "GBT_score = []\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))\n",
    "    GBT_score.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172dcebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = GradientBoostingRegressor(max_depth = 10, n_estimators = 100, learning_rate = 0.1)\n",
    "\n",
    "start = time.time()\n",
    "score = cross_val_score(regressor, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"The score for RandomForest: {}\".format(-np.mean(score)))\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)/10))\n",
    "\n",
    "regressor = regressor.fit(X_train, y_train)\n",
    "y_predict = regressor.predict(X_test)\n",
    "pd.DataFrame(y_predict).to_csv(\"predictions/GradiantBoostingTree_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0f4fb",
   "metadata": {},
   "source": [
    "##### Visualize Models' tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823b03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning Plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.lineplot(x = list(param_grid[0][\"max_depth\"]), y = -np.array(RF_score), label = \"Random Forest Regressor\")\n",
    "sns.scatterplot(x = list(param_grid[0][\"max_depth\"]), y = -np.array(RF_score))\n",
    "sns.lineplot(x = list(param_grid[0][\"max_depth\"]), y = -np.array(DT_score), label = \"Decision Tree Regressor\")\n",
    "sns.scatterplot(x = list(param_grid[0][\"max_depth\"]), y = -np.array(DT_score))\n",
    "sns.lineplot(x = list(param_grid[0][\"max_depth\"]), y = -np.array(GBT_score), label = \"Gradient Boosted Regressor\")\n",
    "sns.scatterplot(x = list(param_grid[0][\"max_depth\"]), y = -np.array(GBT_score))\n",
    "\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"10-fold cross validation MSE\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.title(\"Max_depth Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53e67ee",
   "metadata": {},
   "source": [
    "#### Ensemble Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b6b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57edb08",
   "metadata": {},
   "source": [
    "##### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search(Change param_grid to tune the models)\n",
    "decisionTree = DecisionTreeRegressor(max_depth = 150, min_samples_split = 6)\n",
    "\n",
    "#param_grid = [\n",
    "#    {'n_estimators': [10, 20, 30, 50, 80, 100],\n",
    "#     'learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "#]\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250],\n",
    "     'learning_rate': [1]}\n",
    "]\n",
    "\n",
    "regressor = AdaBoostRegressor(decisionTree)\n",
    "\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de10a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "print(\"The best model is: {} \\n\".format(grid_search.best_estimator_))\n",
    "\n",
    "AB_score = []\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))\n",
    "    AB_score.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-fold cross validation\n",
    "regressor = AdaBoostRegressor(decisionTree, n_estimators = 50, learning_rate = 1)\n",
    "\n",
    "start = time.time()\n",
    "score = cross_val_score(regressor, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"The score for RandomForest: {}\".format(-np.mean(score)))\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)/10))\n",
    "\n",
    "regressor = regressor.fit(X_train, y_train)\n",
    "y_predict = regressor.predict(X_test)\n",
    "pd.DataFrame(y_predict).to_csv(\"predictions/AdaBoost_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a433c01",
   "metadata": {},
   "source": [
    "##### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search (Change param_grid to tune the model)\n",
    "#param_grid = [\n",
    "#    {'n_estimators': [10, 20, 30, 50, 80, 100],\n",
    "#     'learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "#]\n",
    "\n",
    "#param_grid = [{'learning_rate': [0.1], 'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}]\n",
    "\n",
    "param_grid = [{'learning_rate': [0.1], 'n_estimators': [10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250]}]\n",
    "\n",
    "regressor = XGBRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d25348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "print(\"The best model is: {} \\n\".format(grid_search.best_estimator_))\n",
    "\n",
    "XG_score = []\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))\n",
    "    XG_score.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-fold cross validation\n",
    "regressor = XGBRegressor(learning_rate=0.1, n_estimators=900)\n",
    "\n",
    "start = time.time()\n",
    "score = cross_val_score(regressor, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"The score for XGBoost: {}\".format(-np.mean(score)))\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)/10))\n",
    "\n",
    "regressor = regressor.fit(X_train, y_train)\n",
    "y_predict = regressor.predict(X_test)\n",
    "pd.DataFrame(y_predict).to_csv(\"predictions/XGBoost_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ceb0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"predictions\"] = np.array(pd.read_csv(\"D:\\\\kaggle\\\\XGBoost_prediction.csv\")[\"0\"])\n",
    "for i in range(df_test.shape[0]):\n",
    "    if df_test[\"lng\"][i] in list(df_train[\"lng\"]):\n",
    "        temp = df_train[df_train[\"lng\"] == df_test[\"lng\"][i]]\n",
    "        if df_test[\"size_sqft\"][i] in list(temp[\"size_sqft\"]):\n",
    "            temp2 = temp[temp[\"size_sqft\"] == df_test[\"size_sqft\"][i]]\n",
    "            df_test.iloc[i, -1] = np.mean(temp2[\"price\"])\n",
    "for i in range(temp_test.shape[0]):\n",
    "    if temp_test[\"prediction\"][i] == \"null\":\n",
    "        if temp_test[\"lat\"][i] in list(df_train[\"lat\"]):\n",
    "            temp = df_train[df_train[\"lat\"] == temp_test[\"lat\"][i]]\n",
    "            if temp_test[\"size_sqft\"][i] in list(temp[\"size_sqft\"]):\n",
    "                temp2 = temp[temp[\"size_sqft\"] == temp_test[\"size_sqft\"][i]]\n",
    "                temp_test.iloc[i, -1] = np.mean(temp2[\"price\"])\n",
    "pd.DataFrame(df_test[\"predictions\"]).to_csv(\"D:\\\\kaggle\\\\XGBoost_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6d020f",
   "metadata": {},
   "source": [
    "##### LightBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search (Change param_grid to tune the model)\n",
    "\n",
    "#param_grid = [\n",
    "#    {'n_estimators': [10, 20, 30, 50, 80, 100],\n",
    "#     'learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "#]\n",
    "\n",
    "#param_grid = [{'learning_rate': [0.1], 'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}]\n",
    "\n",
    "param_grid = [{'learning_rate': [0.1], 'n_estimators': [10, 30, 50, 70, 90, 110, 130, 150, 170, 190, 210, 230, 250]}]\n",
    "\n",
    "regressor = LGBMRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3137938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "print(\"The best model is: {} \\n\".format(grid_search.best_estimator_))\n",
    "\n",
    "LB_score = []\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))\n",
    "    LB_score.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-fold cross validation\n",
    "regressor = LGBMRegressor(learning_rate=0.1, n_estimators=300)\n",
    "\n",
    "start = time.time()\n",
    "score = cross_val_score(regressor, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"The score for LGBM: {}\".format(-np.mean(score)))\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)/10))\n",
    "\n",
    "regressor = regressor.fit(X_train, y_train)\n",
    "y_predict = regressor.predict(X_test)\n",
    "pd.DataFrame(y_predict).to_csv(\"predictions/LGBM_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc49f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning Plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.lineplot(x = list(param_grid[0][\"n_estimators\"]), y = -np.array(RF_score), label = \"Random Forest Regressor\")\n",
    "sns.scatterplot(x = list(param_grid[0][\"n_estimators\"]), y = -np.array(RF_score))\n",
    "sns.lineplot(x = list(param_grid[0][\"n_estimators\"]), y = -np.array(AB_score), label = \"AdaBoost\")\n",
    "sns.scatterplot(x = list(param_grid[0][\"n_estimators\"]), y = -np.array(AB_score))\n",
    "sns.lineplot(x = list(param_grid[0][\"n_estimators\"]), y = -np.array(XG_score), label = \"XGBoost\")\n",
    "sns.scatterplot(x = list(param_grid[0][\"n_estimators\"]), y = -np.array(XG_score))\n",
    "sns.lineplot(x = list(param_grid[0][\"n_estimators\"]), y = -np.array(LB_score), label = \"LightBoost\")\n",
    "sns.scatterplot(x = list(param_grid[0][\"n_estimators\"]), y = -np.array(LB_score))\n",
    "\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"10-fold cross validation MSE\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.title(\"n_estimators Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d5e769",
   "metadata": {},
   "source": [
    "#### Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a749ef4",
   "metadata": {},
   "source": [
    "As deep learning models always have extremely high time cost while training, we train all deep learning models in the Google Colab Environment with GPU resource and Pytorch Framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4cf70",
   "metadata": {
    "id": "m4BPdn0-o1wj"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch import optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f17485",
   "metadata": {
    "id": "2Q34VNXmpF93"
   },
   "outputs": [],
   "source": [
    "#X_train = pd.read_csv(\"X_train.csv\")\n",
    "#X_test = pd.read_csv(\"X_test.csv\")\n",
    "#y_train = pd.read_csv(\"y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c21b31d",
   "metadata": {
    "id": "OGeaQzQTpPZJ"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.iloc[:, 1:]\n",
    "X_test = X_test.iloc[:, 1:]\n",
    "y_train = y_train.iloc[:, 1:]\n",
    "X_train_N = copy.deepcopy(X_train)\n",
    "X_test_N = copy.deepcopy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f93f0",
   "metadata": {
    "id": "dRzahsz94hmy"
   },
   "outputs": [],
   "source": [
    "for i in list(X_train.columns):\n",
    "    X_train_N[i] = (X_train[i]-min(X_train[i]))/(max(X_train[i]) - min(X_train[i]))\n",
    "for i in list(X_test.columns):\n",
    "    X_test_N[i] = (X_test[i]-min(X_test[i]))/(max(X_test[i]) - min(X_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b79fa",
   "metadata": {
    "id": "14dyZ3qGpqYV"
   },
   "outputs": [],
   "source": [
    "X_train = torch.tensor(np.array(X_train))\n",
    "y_train = torch.tensor(np.array(y_train))\n",
    "X_test = torch.tensor(np.array(X_test))\n",
    "X_train_N = torch.tensor(np.array(X_train_N))\n",
    "X_test_N = torch.tensor(np.array(X_test_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f87e87",
   "metadata": {
    "id": "QB78edLsf5vg"
   },
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k \n",
    "    \n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat((X_train, X_part), dim=0)\n",
    "            y_train = torch.cat((y_train, y_part), dim=0)\n",
    "    #print(X_train.size(),X_valid.size())\n",
    "    return X_train, y_train, X_valid,y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7fcbf",
   "metadata": {
    "id": "7j_TSS6v5lUD"
   },
   "source": [
    "##### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df900e83",
   "metadata": {
    "id": "C8ncUhpb5tla"
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, 1)\n",
    "        self.act = act_layer()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        x = x.squeeze(0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2442db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjEWk4Fm58u-",
    "outputId": "d3462106-8a92-44d1-dbc6-3b881d9981a5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "fold_loss = []\n",
    "learning_process = []\n",
    "val_loss = []\n",
    "time_cost = []\n",
    "\n",
    "for fold in range(k_fold):\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Mlp(in_features = 11).cuda()\n",
    "    criterion=nn.MSELoss()\n",
    "    optimizer=optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)\n",
    "    train_epoch, train_loss = [], []\n",
    "    avg_train_loss_Mlp = []\n",
    "    epoch_time=[]\n",
    "\n",
    "    # Split the 10 folds\n",
    "    X_train_1, y_train_1, X_valid_1, y_valid_1 = get_k_fold_data(k_fold, fold, X_train, y_train)\n",
    "\n",
    "    # Load the data\n",
    "    train_loader = Data.DataLoader(\n",
    "    dataset=Data.TensorDataset(torch.Tensor(X_train_1),y_train_1),      \n",
    "    batch_size=128,      \n",
    "    shuffle=True,               \n",
    "    num_workers=2, \n",
    "    drop_last=True\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    start0 = time.time()\n",
    "    print(\"This is the #{} fold.\".format(fold+1))\n",
    "    for epoch in range(128):\n",
    "        running_loss = 0  \n",
    "        start1 = time.time()\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            start = time.time()\n",
    "            t_image, mask = data[0].cuda(),data[1].cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(t_image) # forward\n",
    "            ###########################################################################\n",
    "            mask=mask.to(torch.float32)\n",
    "            loss = criterion(outputs, mask) # calculate the loss\n",
    "            loss.backward() # back propagation\n",
    "            optimizer.step() # update gradients\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                end = time.time()\n",
    "                print('Epoch {}:[{}/{}], Current Loss: {}, Time: {} ms'.format(epoch+1, i, len(train_loader), loss.item(), end - start))      \n",
    "                train_loss.append(loss.item())\n",
    "                train_epoch.append(str(epoch+1) + '/' + str(i))\n",
    "        end1 = time.time()\n",
    "        print('Epoch {}, train Loss: {:.3f} '.format(epoch+1, running_loss/len(train_loader)), \"Epoch Time: {} ms\".format(end1 - start1))\n",
    "        epoch_time.append(end1-start1)\n",
    "        avg_train_loss_Mlp.append(running_loss/len(train_loader))\n",
    "    learning_process.append(avg_train_loss_Mlp)\n",
    "\n",
    "    model.eval()\n",
    "    X_valid_1 = X_valid_1.cuda()\n",
    "    y_valid_1 = y_valid_1.cuda()\n",
    "    predictions = model(X_valid_1)\n",
    "    valid_loss = criterion(predictions, y_valid_1)\n",
    "    print(\"The #{} fold's cross validation score is : {}\".format(fold, valid_loss))\n",
    "    val_loss.append(float((valid_loss.detach().cpu()).numpy()))\n",
    "    end0 = time.time()\n",
    "    time_cost.append(end0-start0)\n",
    "print(\"The cross_val_score is: {}\".format(np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67b19e",
   "metadata": {
    "id": "fE3F_j_r58x2"
   },
   "outputs": [],
   "source": [
    "Mlp_process = []\n",
    "for i in range(len(learning_process[0])):\n",
    "    temp = []\n",
    "    for j in range(len(learning_process)):\n",
    "        temp.append(learning_process[j][i])\n",
    "    Mlp_process.append(np.mean(temp))\n",
    "\n",
    "Mlp_time = np.mean(time_cost)\n",
    "\n",
    "Mlp_val_loss = np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c06213",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ycgyuL9HhQs",
    "outputId": "b4af5aa4-e9c5-4cf8-82c3-742995dce60f"
   },
   "outputs": [],
   "source": [
    "print(\"The time cost for Mlp is {}\".format(Mlp_time))\n",
    "print(\"The 10-fold MSE score for Mlp is {}\".format(Mlp_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9e7c0",
   "metadata": {
    "id": "Ghyjc2xGKEdE"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"X_test.csv\").iloc[:, 1:]\n",
    "df_test[\"predictions\"] = model(X_test.cuda()).detach().cpu().numpy()\n",
    "df_test[\"predictions\"].to_csv(\"predictions/Mlp_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42b3fa",
   "metadata": {
    "id": "FwvUHKg2OXXk"
   },
   "source": [
    "##### Mlp+Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd397d",
   "metadata": {
    "id": "WpKpxKtuGHJd"
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, act_layer=nn.GELU, drop=0., pred=True):\n",
    "        super().__init__()\n",
    "        #out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.q = nn.Linear(in_features, in_features)\n",
    "        self.k = nn.Linear(in_features, in_features)\n",
    "        self.v = nn.Linear(in_features, in_features)\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.pred = pred\n",
    "        if pred==True:\n",
    "            self.fc2 = nn.Linear(hidden_features,1)\n",
    "        else:\n",
    "            self.fc2 = nn.Linear(hidden_features, in_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        x0 = x\n",
    "        q = self.q(x).unsqueeze(2)\n",
    "        k = self.k(x).unsqueeze(2)\n",
    "        v = self.v(x).unsqueeze(2)\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "        #print(attn.size())\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).squeeze(2)\n",
    "        #print(x.size())\n",
    "        x += x0\n",
    "        x1 = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        if self.pred==False:\n",
    "            x += x1\n",
    "\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mlp_Attn(nn.Module):\n",
    "    def __init__(self, in_features, drop=0.):\n",
    "        super().__init__()\n",
    "        self.Block1 = Mlp(in_features=in_features, hidden_features=64, act_layer=nn.GELU, drop=drop, pred=False)\n",
    "        self.Block2 = Mlp(in_features=in_features, hidden_features=64, act_layer=nn.GELU, drop=drop, pred=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.Block2(self.Block1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c8fab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wpC6sxgGHMC",
    "outputId": "05df9c7f-94f4-4c17-d42d-e6851c90d39a"
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "fold_loss = []\n",
    "learning_process = []\n",
    "val_loss = []\n",
    "time_cost = []\n",
    "\n",
    "for fold in range(k_fold):\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Mlp_Attn(in_features = 11, drop = 0.1).cuda()\n",
    "    criterion=nn.MSELoss()\n",
    "    optimizer=optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)\n",
    "    train_epoch, train_loss = [], []\n",
    "    avg_train_loss_Mlp = []\n",
    "    epoch_time=[]\n",
    "\n",
    "    # Split the 10 folds\n",
    "    X_train_1, y_train_1, X_valid_1, y_valid_1 = get_k_fold_data(k_fold, fold, X_train_N, y_train)\n",
    "\n",
    "    # Load the data\n",
    "    train_loader = Data.DataLoader(\n",
    "    dataset=Data.TensorDataset(torch.Tensor(X_train_1),y_train_1),      \n",
    "    batch_size=128,      \n",
    "    shuffle=True,               \n",
    "    num_workers=2, \n",
    "    drop_last=True\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    start0 = time.time()\n",
    "    print(\"This is the #{} fold.\".format(fold+1))\n",
    "    for epoch in range(128):\n",
    "        running_loss = 0  \n",
    "        start1 = time.time()\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            start = time.time()\n",
    "            t_image, mask = data[0].cuda(),data[1].cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(t_image) # forward\n",
    "            ###########################################################################\n",
    "            mask=mask.to(torch.float32)\n",
    "            loss = criterion(outputs, mask) # calculate the loss\n",
    "            loss.backward() # back propagation\n",
    "            optimizer.step() # update gradients\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                end = time.time()\n",
    "                print('Epoch {}:[{}/{}], Current Loss: {}, Time: {} ms'.format(epoch+1, i, len(train_loader), loss.item(), end - start))      \n",
    "                train_loss.append(loss.item())\n",
    "                train_epoch.append(str(epoch+1) + '/' + str(i))\n",
    "        end1 = time.time()\n",
    "        print('Epoch {}, train Loss: {:.3f} '.format(epoch+1, running_loss/len(train_loader)), \"Epoch Time: {} ms\".format(end1 - start1))\n",
    "        epoch_time.append(end1-start1)\n",
    "        avg_train_loss_Mlp.append(running_loss/len(train_loader))\n",
    "    learning_process.append(avg_train_loss_Mlp)\n",
    "\n",
    "    model.eval()\n",
    "    X_valid_1 = X_valid_1.cuda()\n",
    "    y_valid_1 = y_valid_1.cuda()\n",
    "    predictions = model(X_valid_1)\n",
    "    valid_loss = criterion(predictions, y_valid_1)\n",
    "    print(\"The #{} fold's cross validation score is : {}\".format(fold, valid_loss))\n",
    "    val_loss.append(float((valid_loss.detach().cpu()).numpy()))\n",
    "    end0 = time.time()\n",
    "    time_cost.append(end0-start0)\n",
    "print(\"The cross_val_score is: {}\".format(np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a0eda",
   "metadata": {
    "id": "UCPw_iL6GHOu"
   },
   "outputs": [],
   "source": [
    "Mlp_Attn_process = []\n",
    "for i in range(len(learning_process[0])):\n",
    "    temp = []\n",
    "    for j in range(len(learning_process)):\n",
    "        temp.append(learning_process[j][i])\n",
    "    Mlp_Attn_process.append(np.mean(temp))\n",
    "\n",
    "Mlp_Attn_time = np.mean(time_cost)\n",
    "\n",
    "Mlp_Attn_val_loss = np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ebcbc",
   "metadata": {
    "id": "ulJ3TOg_Lwl3"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"X_test.csv\").iloc[:, 1:]\n",
    "df_test[\"predictions\"] = model(X_test_N.cuda()).detach().cpu().numpy()\n",
    "df_test[\"predictions\"].to_csv(\"predictions/Mlp_ATTN_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b929d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "JIb-d7ixGHRa",
    "outputId": "1f42e37a-bd46-45ce-ae32-82eb5ed91674"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(Mlp_process, label = 'Mlp')\n",
    "plt.plot(Mlp_Attn_process, label = 'Mlp+Attention')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0605b70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-MfYBt7GHUL",
    "outputId": "e07ce283-b69e-489d-f5e8-6d5d9980831d"
   },
   "outputs": [],
   "source": [
    "print(\"The time cost(10-fold cross validation) for Mlp is {}s\".format(Mlp_time * 10))\n",
    "print(\"The time cost(10-fold cross validation) for Mlp+Attention is {}s\".format(Mlp_Attn_time * 10))\n",
    "print(\"The 10-fold cross validation MSE score for Mlp is {}\".format(Mlp_val_loss))\n",
    "print(\"The 10-fold cross validation MSE score for Mlp+Attention is {}\".format(Mlp_Attn_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f1eed",
   "metadata": {
    "id": "jmAIUs4wXZR5"
   },
   "source": [
    "##### Hybrid Model-1(KNN+Random Forest+Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae11c92",
   "metadata": {
    "id": "ONXi67UDGHWv"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ad17b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZoJc-VWzX0hw",
    "outputId": "84e715a9-1318-4b97-bb0c-884770e6293c"
   },
   "outputs": [],
   "source": [
    "decisionTree = DecisionTreeRegressor(max_depth = 150, min_samples_split = 6)\n",
    "regressor_1 = AdaBoostRegressor(decisionTree, learning_rate = 1, n_estimators = 30)\n",
    "regressor_1 = regressor_1.fit(X_train, y_train)\n",
    "y_predict_1_train = regressor_1.predict(X_train)\n",
    "y_predict_1_test = regressor_1.predict(X_test)\n",
    "\n",
    "regressor_2 = RandomForestRegressor(max_depth = 50, n_estimators = 100)\n",
    "regressor_2 = regressor_2.fit(X_train, y_train)\n",
    "y_predict_2_train = regressor_2.predict(X_train)\n",
    "y_predict_2_test = regressor_2.predict(X_test)\n",
    "\n",
    "regressor_3 = KNeighborsRegressor(n_neighbors = 2)\n",
    "regressor_3 = regressor_3.fit(X_train_N, y_train)\n",
    "y_predict_3_train = regressor_3.predict(X_train_N)\n",
    "y_predict_3_test = regressor_3.predict(X_test_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750bf0ae",
   "metadata": {
    "id": "4IEZOUGpX0ki"
   },
   "outputs": [],
   "source": [
    "temp_train = np.concatenate((y_predict_1_train.reshape(-1, 1), y_predict_2_train.reshape(-1, 1), y_predict_3_train.reshape(-1, 1)), axis = 1)\n",
    "temp_test = np.concatenate((y_predict_1_test.reshape(-1, 1), y_predict_2_test.reshape(-1, 1), y_predict_3_test.reshape(-1, 1)), axis = 1)\n",
    "temp_train = torch.tensor(temp_train)\n",
    "temp_test = torch.tensor(temp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd314309",
   "metadata": {
    "id": "iZSq3UUYX0nT"
   },
   "outputs": [],
   "source": [
    "class hybrid_1(nn.Module):\n",
    "    def __init__(self, in_features, act_layer=nn.GELU, drop=0.1, num_regressors = 3):\n",
    "        super().__init__()\n",
    "        self.act = act_layer()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.in_features = in_features\n",
    "        self.fc1 = nn.Linear(3, 3)\n",
    "        self.fc2 = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        predictions = x.to(torch.float32)\n",
    "        predictions = self.fc1(predictions)\n",
    "        predictions = self.act(predictions)\n",
    "        predictions = self.drop(predictions)\n",
    "        prediction = self.fc2(predictions)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de902b04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OOKSFqboX0pr",
    "outputId": "b4f448d9-7057-440e-a25a-20936707ce6b"
   },
   "outputs": [],
   "source": [
    "k_fold = 10\n",
    "fold_loss = []\n",
    "learning_process = []\n",
    "val_loss = []\n",
    "time_cost = []\n",
    "model_list = []\n",
    "\n",
    "for fold in range(k_fold):\n",
    "\n",
    "    # Initialize the model\n",
    "    model = hybrid_1(in_features = 11, drop = 0.1).cuda()\n",
    "    criterion=nn.MSELoss()\n",
    "    optimizer=optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0)\n",
    "    train_epoch, train_loss = [], []\n",
    "    avg_train_loss_Hybrid = []\n",
    "    epoch_time=[]\n",
    "\n",
    "    # Split the 10 folds\n",
    "    X_train_1, y_train_1, X_valid_1, y_valid_1 = get_k_fold_data(k_fold, fold, temp_train, y_train)\n",
    "\n",
    "    # Load the data\n",
    "    train_loader = Data.DataLoader(\n",
    "    dataset=Data.TensorDataset(torch.Tensor(X_train_1),y_train_1),      \n",
    "    batch_size=128,      \n",
    "    shuffle=True,               \n",
    "    num_workers=2, \n",
    "    drop_last=True\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "    start0 = time.time()\n",
    "    print(\"This is the #{} fold.\".format(fold+1))\n",
    "    for epoch in range(128):\n",
    "        running_loss = 0  \n",
    "        start1 = time.time()\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            start = time.time()\n",
    "            t_image, mask = data[0].cuda(),data[1].cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(t_image) # forward\n",
    "            ###########################################################################\n",
    "            mask=mask.to(torch.float32)\n",
    "            loss = criterion(outputs, mask) # calculate the loss\n",
    "            loss.backward() # back propagation\n",
    "            optimizer.step() # update gradients\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                end = time.time()\n",
    "                print('Epoch {}:[{}/{}], Current Loss: {}, Time: {} ms'.format(epoch+1, i, len(train_loader), loss.item(), end - start))      \n",
    "                train_loss.append(loss.item())\n",
    "                train_epoch.append(str(epoch+1) + '/' + str(i))\n",
    "        end1 = time.time()\n",
    "        print('Epoch {}, train Loss: {:.3f} '.format(epoch+1, running_loss/len(train_loader)), \"Epoch Time: {} ms\".format(end1 - start1))\n",
    "        epoch_time.append(end1-start1)\n",
    "        avg_train_loss_Hybrid.append(running_loss/len(train_loader))\n",
    "    learning_process.append(avg_train_loss_Hybrid)\n",
    "\n",
    "    model.eval()\n",
    "    X_valid_1 = X_valid_1.cuda()\n",
    "    y_valid_1 = y_valid_1.cuda()\n",
    "    predictions = model(X_valid_1)\n",
    "    valid_loss = criterion(predictions, y_valid_1)\n",
    "    print(\"The #{} fold's cross validation score is : {}\".format(fold, valid_loss))\n",
    "    val_loss.append(float((valid_loss.detach().cpu()).numpy()))\n",
    "    end0 = time.time()\n",
    "    time_cost.append(end0-start0)\n",
    "    model_list.append(model)\n",
    "print(\"The cross_val_score is: {}\".format(np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8eb7a9",
   "metadata": {
    "id": "X8iGQ7DYGHbx"
   },
   "outputs": [],
   "source": [
    "Hybrid_process = []\n",
    "for i in range(len(learning_process[0])):\n",
    "    temp = []\n",
    "    for j in range(len(learning_process)):\n",
    "        temp.append(learning_process[j][i])\n",
    "    Hybrid_process.append(np.mean(temp))\n",
    "\n",
    "Hybrid_time = np.mean(time_cost)\n",
    "\n",
    "Hybrid_val_loss = np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13d481",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "id": "85Q1JO6vGHfP",
    "outputId": "3be5589a-a4ee-4902-be1c-6b49af574822"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(Hybrid_process)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b711a28a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g2woJPMn580v",
    "outputId": "e202dfa5-c5a6-4a54-99aa-2c8961dac931"
   },
   "outputs": [],
   "source": [
    "print(\"The time cost(10-fold cross validation) for Hybrid Model is {}s\".format(Hybrid_time * 10))\n",
    "print(\"The 10-fold cross validation MSE score for Hybrid Model is {}\".format(Hybrid_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d789dd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "rIRke2-m584F",
    "outputId": "84f35c7e-13e4-4aa9-945e-b4ddd744ace2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(val_loss)\n",
    "plt.xlabel(\"#Model\")\n",
    "plt.ylabel(\"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554b6b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ed9oDBC-TRor",
    "outputId": "7b35b099-1e7e-4cf2-f6dc-5fe8e8944304"
   },
   "outputs": [],
   "source": [
    "eva_list_hybrid = []\n",
    "for i in range(k_fold):\n",
    "  eva_list_hybrid.append(float(criterion(model_list[i](temp_train.cuda()), y_train.cuda()).detach().cpu().numpy()))\n",
    "  print(\"The {}-th model's loss on training set is {}.\".format(i+1, eva_list_hybrid[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442d5dc",
   "metadata": {
    "id": "56dy1L-si8bq"
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(model_list[4](temp_test.cuda()).detach().cpu().numpy()).to_csv(\"hybrid_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386dfc9",
   "metadata": {
    "id": "xeaBPczhkB3u"
   },
   "source": [
    "##### Hybrid Model-2(KNN+AdaBoost+Random Forest+Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1890cd4",
   "metadata": {
    "id": "KNm5caXMj1bk"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"X_train.csv\").iloc[:, 1:]\n",
    "df_train[\"price\"] = pd.read_csv(\"y_train.csv\").iloc[:, 1:]\n",
    "df_test = pd.read_csv(\"X_test.csv\").iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ca711",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "yBECfytwmrdT",
    "outputId": "22cc3595-8002-48f8-8382-d1041f689713"
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b05ff0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "dAYzNX3vl1ju",
    "outputId": "b1842a65-6423-45be-e53f-5e17e45a585f"
   },
   "outputs": [],
   "source": [
    "df_test[\"predictions\"] = model_list[1](temp_test.cuda()).detach().cpu().numpy()\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d14bc",
   "metadata": {
    "id": "E4lCJ_JBmJrn"
   },
   "outputs": [],
   "source": [
    "for i in range(df_test.shape[0]):\n",
    "    if df_test[\"5\"][i] in list(df_train[\"lng\"]):\n",
    "        temp = df_train[df_train[\"lng\"] == df_test[\"5\"][i]]\n",
    "        if df_test[\"3\"][i] in list(temp[\"size_sqft\"]):\n",
    "            temp2 = temp[temp[\"size_sqft\"] == df_test[\"3\"][i]]\n",
    "            df_test.iloc[i, -1] = np.mean(temp2[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0779a",
   "metadata": {
    "id": "1qo_64zcnKYe"
   },
   "outputs": [],
   "source": [
    "for i in range(df_test.shape[0]):\n",
    "    if df_test[\"4\"][i] in list(df_train[\"lat\"]):\n",
    "        temp = df_train[df_train[\"lat\"] == df_test[\"4\"][i]]\n",
    "        if df_test[\"3\"][i] in list(temp[\"size_sqft\"]):\n",
    "            temp2 = temp[temp[\"size_sqft\"] == df_test[\"3\"][i]]\n",
    "            df_test.iloc[i, -1] = np.mean(temp2[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c77340",
   "metadata": {
    "id": "-R3s0TQmnaE2"
   },
   "outputs": [],
   "source": [
    "df_test[\"predictions\"].to_csv(\"predictions/Hybrid_2_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f2ce7",
   "metadata": {},
   "source": [
    "#### Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac932ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0dc02",
   "metadata": {},
   "source": [
    "##### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f637dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search(Change the param_grid to tune the parameters)\n",
    "\n",
    "param_grid = [{'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "\n",
    "regressor = KNeighborsRegressor()\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_N, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d68df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "print(\"The best model is: {} \\n\".format(grid_search.best_estimator_))\n",
    "\n",
    "KNN_score = []\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))\n",
    "    KNN_score.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f15d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10-fold cross validation\n",
    "regressor = KNeighborsRegressor(n_neighbors = 2)\n",
    "\n",
    "start = time.time()\n",
    "score = cross_val_score(regressor, X_train_N, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"The score for KNN: {}\".format(-np.mean(score)))\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)/10))\n",
    "\n",
    "regressor = regressor.fit(X_train_N, y_train)\n",
    "y_predict = regressor.predict(X_test_N)\n",
    "pd.DataFrame(y_predict).to_csv(\"predictions/KNN_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8232b",
   "metadata": {},
   "source": [
    "##### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64a9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search(Change the param_grid to tune the parameters)\n",
    "\n",
    "param_grid = [{'C': np.linspace(1, 10000000, 50)}]\n",
    "\n",
    "regressor = LinearSVR(max_iter=1000000000)\n",
    "\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=10, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_N, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b5efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "print(\"The best model is: {} \\n\".format(grid_search.best_estimator_))\n",
    "\n",
    "SVR_score = []\n",
    "\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean,param in zip(means,params):\n",
    "    print(\"%f  with:   %r\" % (mean,param))\n",
    "    SVR_score.append(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c584d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearSVR(max_iter=1000000000, C = 9795918)\n",
    "\n",
    "start = time.time()\n",
    "score = cross_val_score(regressor, X_train_N, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "print(\"The score for SVR: {}\".format(-np.mean(score)))\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)/10))\n",
    "\n",
    "regressor = regressor.fit(X_train_N, y_train)\n",
    "y_predict = regressor.predict(X_test_N)\n",
    "pd.DataFrame(y_predict).to_csv(\"predictions/SVR_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning Plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.lineplot(x = list(param_grid[0][\"C\"]), y = -np.array(SVR_score), label = \"SVR\")\n",
    "sns.scatterplot(x = list(param_grid[0][\"C\"]), y = -np.array(SVR_score))\n",
    "\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"10-fold cross validation MSE\")\n",
    "plt.legend(loc = 'best')\n",
    "plt.title(\"C Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d81c3ad",
   "metadata": {},
   "source": [
    "##### KNN Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test = copy.deepcopy(X_test)\n",
    "temp_test[\"prediction\"] = \"null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8290180",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for i in range(temp_test.shape[0]):\n",
    "    if temp_test[\"lng\"][i] in list(df_train[\"lng\"]):\n",
    "        temp = df_train[df_train[\"lng\"] == temp_test[\"lng\"][i]]\n",
    "        if temp_test[\"size_sqft\"][i] in list(temp[\"size_sqft\"]):\n",
    "            temp2 = temp[temp[\"size_sqft\"] == temp_test[\"size_sqft\"][i]]\n",
    "            temp_test.iloc[i, -1] = np.mean(temp2[\"price\"])\n",
    "    if temp_test[\"prediction\"][i] == \"null\":\n",
    "        if temp_test[\"lat\"][i] in list(df_train[\"lat\"]):\n",
    "            temp = df_train[df_train[\"lat\"] == temp_test[\"lat\"][i]]\n",
    "            if temp_test[\"size_sqft\"][i] in list(temp[\"size_sqft\"]):\n",
    "                temp2 = temp[temp[\"size_sqft\"] == temp_test[\"size_sqft\"][i]]\n",
    "                temp_test.iloc[i, -1] = np.mean(temp2[\"price\"])\n",
    "    if temp_test[\"prediction\"][i] == \"null\":\n",
    "        temp = df_train[df_train[\"property_type\"] == temp_test[\"property_type\"][i]]\n",
    "        lat = temp_test[\"lat\"][i]\n",
    "        lng = temp_test[\"lng\"][i]\n",
    "        index = np.argmin(np.sqrt((df_train[\"lat\"]-lat)**2+(df_train[\"lng\"]-lng)**2))\n",
    "        temp_test.iloc[i, -1] = (df_train[\"price\"][index]/df_train[\"size_sqft\"][index]) * temp_test[\"size_sqft\"][i]\n",
    "end = time.time()\n",
    "print(\"Time for 1 fold: {}\".format((end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac66c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_predict).to_csv(\"predictions/KNN_V_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.subplots(figsize = (12,12))\n",
    "sns.heatmap(train.corr(),annot = True,vmax = 1,square = True,cmap = \"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f8d683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
